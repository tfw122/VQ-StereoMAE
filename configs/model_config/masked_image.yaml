model_config:
  name: masked_image_autoencoder_msg_gan # versions of masked_image_autoencoders

  # ----- image encoder architecture -----
  # base encoder: embed_dim=768, depth=12, num_heads=12
  # large encoder: embed_dim=1024, depth=24, num_heads=16
  image_encoder:
    patch_size: [16, 32]  # can't use a standard square patch on a non-square image
    in_channels: 3 
    embed_dim: 768 # if using base vit model, embed_dim = 768; if using large, embed_dim=1024
    depth: 12 # num layers of vit, for base: 12, for large: 24
    mlp_ratio: 4
    num_heads: 12  # for base, num_heads=12; for large, num_heads=16

  image_decoder:
    decoder_embed_dim: 512
    decoder_depth: 8
    decoder_num_heads: 16

  image_loss_weightings:
    ssim: 0.85
    style: 40.0
    perc: 0.05
    gan: 0.7 
    l1: 1.0

  discriminator:
    depth: 6 # no. of conv blocks in the discriminator, 6 should be max; otherwise the image will be as small as 3x3 for which loss will be too shallow (almost zero)
    conv_layer_type: default #equal / default
    feature_size_ndf: 512
    input_channels_nc: 3
    gan_arch_type: msg # using single scale / MSG (multi scale) discriminator architecture. the args above are specifically for MSG GAN
    # for single scale; architecture is set (similar to DC GAN). Change arh gan_arch_type to ensure loss functions run correctly.

  dall_e:
    model_dir: /tiaW/data/pretrained_perceptual/dall_e
    # if model weights not stored; use the following link to store: 
    # https://cdn.openai.com/dall-e/decoder.pkl
    # https://cdn.openai.com/dall-e/encoder.pkl
    
  normalisation_params: imagenet  
  norm_layer_arg: partial
  mask_ratio: 0.75 # Masking ratio (percentage of removed patches).
  loss_type: perceptual # mae, ssim, perceptual, lpips, ssim_perceptual
  feature_extractor: vgg # only applicable for the perceptual loss; uses VGG16 as default
  norm_pix_loss: False # Use (per-patch) normalized pixels as targets for computing loss (NOT NEEDED though; works fine without)
  

  ckpt_monitor: val_loss
  tokenizer_model : vqkd_encoder_base_decoder_1x768x12_clip
  codebook_size: 8192
  codebook_dim: 32

  finetune_imagenet: null
  # finetune using imagenet weights, arg= path to the local directory with imagenet weights 
  # make arg: null if you do not want to finetune; you will be pretraining from scratch (rand weights)
  # to use this; ensure pretrained weights from https://github.com/facebookresearch/mae
  # are downloaded and the path to the relevant directory is provided

  num_samples_to_visualise: 1 # number of samples to visualise in callbacks
  frequency_to_visualise: 6000
  # load checkpoint; 
  load_checkpoint: null

  drop_path_rate: 0.1
  qkv_bias: True
  qk_scale: None
  drop_rate: 0.1
  attn_drop_rate: 0.0
  norm_layer: partial
  init_values: 1e-4
  use_rel_pos_bias: False
  attn_head_dim: 64
  act_layer: gelu